{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b115c5a",
   "metadata": {},
   "source": [
    "# TPOTを用いてSIGNATEの楽曲のジャンル推定チャレンジ！に挑戦する\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a59dbc",
   "metadata": {},
   "source": [
    "## ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0022464",
   "metadata": {},
   "outputs": [],
   "source": [
    "!-m pip install --upgrade pip\n",
    "! pip install sklearn\n",
    "! pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7cc46eff-ea57-4e9e-9f90-151244e6db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tpot\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.metrics import confusion_matrix#混同行列(誤差行列)の表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78dbec8",
   "metadata": {},
   "source": [
    "## 各変数の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "87e1b3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データのラベル\n",
    "train_master_path = \"/Users/oonoharuki/competition/SIGNATE/SOTAchallenge/楽曲のジャンル推定チャレンジ！！/train.csv\"\n",
    "#テストデータのラベル\n",
    "test_master_path = \"/Users/oonoharuki/competition/SIGNATE/SOTAchallenge/楽曲のジャンル推定チャレンジ！！/test02.csv\"\n",
    "#保存作のパス\n",
    "submit_master_path = \"/Users/oonoharuki/competition/SIGNATE/SOTAchallenge/楽曲のジャンル推定チャレンジ！！/submit.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182adfa8",
   "metadata": {},
   "source": [
    "## データの読み込みと前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8334d071-be9a-48cf-88b8-2cae2cecd509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習用画像データの読み込み\n",
    "train = pd.read_csv(train_master_path)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18afbbf5-825d-4fe9-97d5-9863f72b701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index部分の削除\n",
    "train = train.iloc[:,1:]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f20fca9-1261-4335-8882-08d4e7fc0e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#重複しているカラムがあるのかの確認\n",
    "train.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ed6ae011-3a8d-46ea-a574-9ccf4e95f99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4046 entries, 0 to 4045\n",
      "Data columns (total 42 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   genre             4046 non-null   int64  \n",
      " 1   popularity        4046 non-null   int64  \n",
      " 2   duration_ms       4046 non-null   int64  \n",
      " 3   acousticness      4046 non-null   float64\n",
      " 4   positiveness      4036 non-null   float64\n",
      " 5   danceability      4038 non-null   float64\n",
      " 6   loudness          4046 non-null   float64\n",
      " 7   energy            4046 non-null   float64\n",
      " 8   liveness          4043 non-null   float64\n",
      " 9   speechiness       4038 non-null   float64\n",
      " 10  instrumentalness  4045 non-null   float64\n",
      " 11  tempo_121-152     4046 non-null   uint8  \n",
      " 12  tempo_153-176     4046 non-null   uint8  \n",
      " 13  tempo_177-192     4046 non-null   uint8  \n",
      " 14  tempo_193-208     4046 non-null   uint8  \n",
      " 15  tempo_209-220     4046 non-null   uint8  \n",
      " 16  tempo_41-50       4046 non-null   uint8  \n",
      " 17  tempo_51-56       4046 non-null   uint8  \n",
      " 18  tempo_57-63       4046 non-null   uint8  \n",
      " 19  tempo_64-76       4046 non-null   uint8  \n",
      " 20  tempo_77-96       4046 non-null   uint8  \n",
      " 21  tempo_97-120      4046 non-null   uint8  \n",
      " 22  region_region_B   4046 non-null   uint8  \n",
      " 23  region_region_C   4046 non-null   uint8  \n",
      " 24  region_region_D   4046 non-null   uint8  \n",
      " 25  region_region_E   4046 non-null   uint8  \n",
      " 26  region_region_F   4046 non-null   uint8  \n",
      " 27  region_region_G   4046 non-null   uint8  \n",
      " 28  region_region_H   4046 non-null   uint8  \n",
      " 29  region_region_I   4046 non-null   uint8  \n",
      " 30  region_region_J   4046 non-null   uint8  \n",
      " 31  region_region_K   4046 non-null   uint8  \n",
      " 32  region_region_L   4046 non-null   uint8  \n",
      " 33  region_region_M   4046 non-null   uint8  \n",
      " 34  region_region_N   4046 non-null   uint8  \n",
      " 35  region_region_O   4046 non-null   uint8  \n",
      " 36  region_region_P   4046 non-null   uint8  \n",
      " 37  region_region_Q   4046 non-null   uint8  \n",
      " 38  region_region_R   4046 non-null   uint8  \n",
      " 39  region_region_S   4046 non-null   uint8  \n",
      " 40  region_region_T   4046 non-null   uint8  \n",
      " 41  region_unknown    4046 non-null   uint8  \n",
      "dtypes: float64(8), int64(3), uint8(31)\n",
      "memory usage: 470.3 KB\n"
     ]
    }
   ],
   "source": [
    "#train内の情報の確認\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85500e7-8aa2-4865-af26-0a5db8cd9af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dtypeを揃えるため、カテゴリ変数をダミー変数に変換\n",
    "train = pd.get_dummies(train,drop_first=True)\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bd0fbd-da78-4c68-ae8d-e761920fa2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#今回の目的変数であるgenreの除去\n",
    "x = train.drop(\"genre\",axis = 1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bf45ad3f-4682-46f8-8a00-5f64a869390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#目的変数genreを設定する\n",
    "y = train[\"genre\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e19b52d0-cd53-473c-a500-dda5baf87fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test_splitを用いて訓練用とテスト用に分割する\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,train_size=0.6,random_state=0,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864675f0",
   "metadata": {},
   "source": [
    "## 学習モデルの作成\n",
    "[こちら](http://epistasislab.github.io/tpot/api/)を参照"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "70a90289-7b47-4756-b801-24d117090160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 operators have been imported by TPOT.\n",
      "Imputing missing values in feature set\n",
      "Optimization Progress:   0%|          | 0/4970 [00:00<?, ?pipeline/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oonoharuki/.pyenv/versions/3.9.4/lib/python3.9/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n",
      "/Users/oonoharuki/.pyenv/versions/3.9.4/lib/python3.9/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n",
      "/Users/oonoharuki/.pyenv/versions/3.9.4/lib/python3.9/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n",
      "/Users/oonoharuki/.pyenv/versions/3.9.4/lib/python3.9/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "                                                                                  \n",
      "Generation 1 - Current Pareto front scores:\n",
      "                                                                                  \n",
      "-1\t0.6205116456662848\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.8500000000000001, GradientBoostingClassifier__min_samples_leaf=10, GradientBoostingClassifier__min_samples_split=10, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8500000000000001)\n",
      "                                                                                  \n",
      "-2\t0.6609002587925842\tExtraTreesClassifier(MLPClassifier(input_matrix, MLPClassifier__alpha=0.0001, MLPClassifier__learning_rate_init=1.0), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=10, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                  \n",
      "Generation 2 - Current Pareto front scores:\n",
      "                                                                                  \n",
      "-1\t0.6625455008272878\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=10, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Skipped pipeline #265 due to time out. Continuing to the next pipeline.           \n",
      "Skipped pipeline #280 due to time out. Continuing to the next pipeline.           \n",
      "                                                                                  \n",
      "Generation 3 - Current Pareto front scores:\n",
      "                                                                                  \n",
      "-1\t0.6625455008272878\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=10, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Optimization Progress:   6%|▌         | 284/4970 [54:24<22:00:07, 16.90s/pipeline]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oonoharuki/.pyenv/versions/3.9.4/lib/python3.9/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                    \n",
      "Generation 4 - Current Pareto front scores:\n",
      "                                                                                    \n",
      "-1\t0.6625455008272878\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=10, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "Skipped pipeline #361 due to time out. Continuing to the next pipeline.             \n",
      "                                                                                    \n",
      "Generation 5 - Current Pareto front scores:\n",
      "                                                                                    \n",
      "-1\t0.6625455008272878\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=10, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                    \n",
      "-2\t0.6641881973611641\tExtraTreesClassifier(MLPClassifier(input_matrix, MLPClassifier__alpha=0.0001, MLPClassifier__learning_rate_init=1.0), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=10, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "Skipped pipeline #468 due to time out. Continuing to the next pipeline.             \n",
      "                                                                                    \n",
      "Generation 6 - Current Pareto front scores:\n",
      "                                                                                    \n",
      "-1\t0.6625455008272878\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=10, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                    \n",
      "-2\t0.6641881973611641\tExtraTreesClassifier(MLPClassifier(input_matrix, MLPClassifier__alpha=0.0001, MLPClassifier__learning_rate_init=1.0), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=10, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Skipped pipeline #504 due to time out. Continuing to the next pipeline.             \n",
      "Skipped pipeline #545 due to time out. Continuing to the next pipeline.             \n",
      "                                                                                    \n",
      "Generation 7 - Current Pareto front scores:\n",
      "                                                                                    \n",
      "-1\t0.6625455008272878\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=10, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                    \n",
      "-2\t0.6641881973611641\tExtraTreesClassifier(MLPClassifier(input_matrix, MLPClassifier__alpha=0.0001, MLPClassifier__learning_rate_init=1.0), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=10, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "Skipped pipeline #580 due to time out. Continuing to the next pipeline.             \n",
      "                                                                                    \n",
      "Generation 8 - Current Pareto front scores:\n",
      "                                                                                    \n",
      "-1\t0.6625455008272878\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=10, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                    \n",
      "-2\t0.6641881973611641\tExtraTreesClassifier(MLPClassifier(input_matrix, MLPClassifier__alpha=0.0001, MLPClassifier__learning_rate_init=1.0), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=10, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "                                                                                    \n",
      "Generation 9 - Current Pareto front scores:\n",
      "                                                                                    \n",
      "-1\t0.6625455008272878\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=10, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                    \n",
      "-2\t0.6745000212125069\tGradientBoostingClassifier(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=19, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                    \n",
      "Generation 10 - Current Pareto front scores:\n",
      "                                                                                    \n",
      "-1\t0.6625455008272878\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=10, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                    \n",
      "-2\t0.6745000212125069\tGradientBoostingClassifier(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=19, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "                                                                                    \n",
      "Generation 11 - Current Pareto front scores:\n",
      "                                                                                    \n",
      "-1\t0.6625455008272878\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=10, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                    \n",
      "-2\t0.6745000212125069\tGradientBoostingClassifier(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=19, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                    \n",
      "Generation 12 - Current Pareto front scores:\n",
      "                                                                                    \n",
      "-1\t0.6740825590768317\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=7, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                    \n",
      "-2\t0.6745000212125069\tGradientBoostingClassifier(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=19, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Skipped pipeline #948 due to time out. Continuing to the next pipeline.             \n",
      "                                                                                    \n",
      "Generation 13 - Current Pareto front scores:\n",
      "                                                                                    \n",
      "-1\t0.6740825590768317\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=7, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                    \n",
      "-2\t0.6745000212125069\tGradientBoostingClassifier(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=19, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "                                                                                     \n",
      "Generation 14 - Current Pareto front scores:\n",
      "                                                                                    \n",
      "-1\t0.6753222179797207\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "Skipped pipeline #1089 due to time out. Continuing to the next pipeline.             \n",
      "                                                                                     \n",
      "Generation 15 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6753222179797207\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.6757320436129142\tGradientBoostingClassifier(CombineDFs(input_matrix, ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.8500000000000001, GradientBoostingClassifier__min_samples_leaf=10, GradientBoostingClassifier__min_samples_split=4, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "Skipped pipeline #1163 due to time out. Continuing to the next pipeline.             \n",
      "                                                                                     \n",
      "Generation 16 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6753222179797207\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.6798532094522931\tExtraTreesClassifier(MLPClassifier(input_matrix, MLPClassifier__alpha=0.0001, MLPClassifier__learning_rate_init=0.5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=10, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                     \n",
      "Generation 17 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.676550846379025\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.6798532094522931\tExtraTreesClassifier(MLPClassifier(input_matrix, MLPClassifier__alpha=0.0001, MLPClassifier__learning_rate_init=0.5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=10, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Skipped pipeline #1294 due to time out. Continuing to the next pipeline.             \n",
      "Skipped pipeline #1312 due to time out. Continuing to the next pipeline.             \n",
      "                                                                                     \n",
      "Generation 18 - Current Pareto front scores:\n",
      "                                                                                    \n",
      "-1\t0.6777922022824658\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=10, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                    \n",
      "-2\t0.6798532094522931\tExtraTreesClassifier(MLPClassifier(input_matrix, MLPClassifier__alpha=0.0001, MLPClassifier__learning_rate_init=0.5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=10, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                     \n",
      "Generation 19 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6790293156845276\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=7, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.6798532094522931\tExtraTreesClassifier(MLPClassifier(input_matrix, MLPClassifier__alpha=0.0001, MLPClassifier__learning_rate_init=0.5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=10, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-3\t0.686032836960672\tGradientBoostingClassifier(ExtraTreesClassifier(MinMaxScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.8500000000000001, GradientBoostingClassifier__min_samples_leaf=10, GradientBoostingClassifier__min_samples_split=4, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "                                                                                     \n",
      "Generation 20 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6790293156845276\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=7, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.6810835348521488\tGradientBoostingClassifier(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.8500000000000001, GradientBoostingClassifier__min_samples_leaf=10, GradientBoostingClassifier__min_samples_split=4, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8500000000000001)\n",
      "                                                                                     \n",
      "-3\t0.686032836960672\tGradientBoostingClassifier(ExtraTreesClassifier(MinMaxScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.8500000000000001, GradientBoostingClassifier__min_samples_leaf=10, GradientBoostingClassifier__min_samples_split=4, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "Skipped pipeline #1489 due to time out. Continuing to the next pipeline.             \n",
      "                                                                                     \n",
      "Generation 21 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6790293156845276\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=7, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.690561282932417\tExtraTreesClassifier(StandardScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only dual=False, got dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                     \n",
      "Generation 22 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6876797759959272\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.690561282932417\tExtraTreesClassifier(StandardScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Skipped pipeline #1634 due to time out. Continuing to the next pipeline.             \n",
      "Skipped pipeline #1656 due to time out. Continuing to the next pipeline.             \n",
      "Skipped pipeline #1669 due to time out. Continuing to the next pipeline.             \n",
      "Skipped pipeline #1681 due to time out. Continuing to the next pipeline.             \n",
      "                                                                                     \n",
      "Generation 23 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6876797759959272\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.690561282932417\tExtraTreesClassifier(StandardScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                     \n",
      "Generation 24 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6876797759959272\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.690561282932417\tExtraTreesClassifier(StandardScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Skipped pipeline #1777 due to time out. Continuing to the next pipeline.             \n",
      "                                                                                     \n",
      "Generation 25 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6876797759959272\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.690561282932417\tExtraTreesClassifier(StandardScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                     \n",
      "Generation 26 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6876797759959272\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.690561282932417\tExtraTreesClassifier(StandardScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                     \n",
      "Generation 27 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6876797759959272\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.6950965169063681\tExtraTreesClassifier(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Skipped pipeline #2007 due to time out. Continuing to the next pipeline.             \n",
      "                                                                                     \n",
      "Generation 28 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6876797759959272\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.6950965169063681\tExtraTreesClassifier(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                     \n",
      "Generation 29 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6876797759959272\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.6950965169063681\tExtraTreesClassifier(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "                                                                                      \n",
      "Generation 30 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6876797759959272\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.6950965169063681\tExtraTreesClassifier(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "Skipped pipeline #2230 due to time out. Continuing to the next pipeline.              \n",
      "                                                                                      \n",
      "Generation 31 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6876797759959272\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.6950965169063681\tExtraTreesClassifier(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                      \n",
      "Generation 32 - Current Pareto front scores:\n",
      "                                                                                      \n",
      "-1\t0.6876797759959272\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                      \n",
      "-2\t0.6950965169063681\tExtraTreesClassifier(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                      \n",
      "Generation 33 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6876797759959272\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.6950965169063681\tExtraTreesClassifier(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-3\t0.6967375164396928\tExtraTreesClassifier(VarianceThreshold(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.45, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), VarianceThreshold__threshold=0.0005), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "                                                                                      \n",
      "Generation 34 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.6950965169063681\tExtraTreesClassifier(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-3\t0.6967375164396928\tExtraTreesClassifier(VarianceThreshold(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.45, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), VarianceThreshold__threshold=0.0005), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                     \n",
      "Generation 35 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.6950965169063681\tExtraTreesClassifier(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-3\t0.6967375164396928\tExtraTreesClassifier(VarianceThreshold(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.45, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), VarianceThreshold__threshold=0.0005), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                     \n",
      "Generation 36 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.6950965169063681\tExtraTreesClassifier(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-3\t0.6971515845742651\tExtraTreesClassifier(RobustScaler(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.45, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix)), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "                                                                                     \n",
      "Generation 37 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.6959195621738579\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.9000000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-3\t0.6971515845742651\tExtraTreesClassifier(RobustScaler(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.45, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix)), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                     \n",
      "Generation 38 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.6959195621738579\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.9000000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-3\t0.6971515845742651\tExtraTreesClassifier(RobustScaler(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.45, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix)), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "                                                                                     \n",
      "Generation 39 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.6959195621738579\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.9000000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-3\t0.6971515845742651\tExtraTreesClassifier(RobustScaler(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.45, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix)), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "                                                                                     \n",
      "Generation 40 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.6959195621738579\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.9000000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-3\t0.6971626150778499\tExtraTreesClassifier(RobustScaler(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.45, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix)), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "                                                                                     \n",
      "Generation 41 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.695922107674685\tExtraTreesClassifier(CombineDFs(input_matrix, SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.45, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1)), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-3\t0.6988036146111748\tExtraTreesClassifier(MLPClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.45, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), MLPClassifier__alpha=0.001, MLPClassifier__learning_rate_init=1.0), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                     \n",
      "Generation 42 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.695922107674685\tExtraTreesClassifier(CombineDFs(input_matrix, SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.45, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1)), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-3\t0.6988036146111748\tExtraTreesClassifier(MLPClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.45, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), MLPClassifier__alpha=0.001, MLPClassifier__learning_rate_init=1.0), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "                                                                                     \n",
      "Generation 43 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.6971541300750923\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.3, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-3\t0.6988036146111748\tExtraTreesClassifier(MLPClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.45, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), MLPClassifier__alpha=0.001, MLPClassifier__learning_rate_init=1.0), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                     \n",
      "Generation 44 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.6971541300750923\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.3, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-3\t0.6988036146111748\tExtraTreesClassifier(MLPClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.45, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), MLPClassifier__alpha=0.001, MLPClassifier__learning_rate_init=1.0), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "                                                                                     \n",
      "Generation 45 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.6971541300750923\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.3, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-3\t0.6988036146111748\tExtraTreesClassifier(MLPClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.45, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), MLPClassifier__alpha=0.001, MLPClassifier__learning_rate_init=1.0), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                     \n",
      "Generation 46 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.6971583725764712\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.45, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-3\t0.6988036146111748\tExtraTreesClassifier(MLPClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.45, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), MLPClassifier__alpha=0.001, MLPClassifier__learning_rate_init=1.0), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _mate_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _mate_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _mate_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.                               \n",
      "                                                                                     \n",
      "Generation 47 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.6971583725764712\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.45, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-3\t0.6988036146111748\tExtraTreesClassifier(MLPClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.45, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), MLPClassifier__alpha=0.001, MLPClassifier__learning_rate_init=1.0), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "                                                                                     \n",
      "Generation 48 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.6971583725764712\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.45, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-3\t0.6988036146111748\tExtraTreesClassifier(MLPClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.45, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), MLPClassifier__alpha=0.001, MLPClassifier__learning_rate_init=1.0), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                     \n",
      "Generation 49 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.6971583725764712\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.45, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-3\t0.6988061601120021\tExtraTreesClassifier(CombineDFs(VarianceThreshold(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.3, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), VarianceThreshold__threshold=0.01), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-4\t0.70045055364643\tExtraTreesClassifier(CombineDFs(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.45, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), ZeroCount(input_matrix)), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "                                                                                     \n",
      "Generation 50 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.6971583725764712\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.45, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-3\t0.6988061601120021\tExtraTreesClassifier(CombineDFs(VarianceThreshold(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.3, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), VarianceThreshold__threshold=0.01), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-4\t0.70045055364643\tExtraTreesClassifier(CombineDFs(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.45, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), ZeroCount(input_matrix)), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                     \n",
      "Generation 51 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.6971583725764712\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.45, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-3\t0.7021025836833397\tExtraTreesClassifier(ZeroCount(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.45, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix)), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                     \n",
      "Generation 52 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.7041618938526156\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.15000000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _mate_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "                                                                                     \n",
      "Generation 53 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.7041618938526156\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.15000000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "                                                                                     \n",
      "Generation 54 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.7041618938526156\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.15000000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                     \n",
      "Generation 55 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.7041618938526156\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.15000000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                     \n",
      "Generation 56 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.7041618938526156\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.15000000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.                               \n",
      "                                                                                     \n",
      "Generation 57 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.7041618938526156\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.15000000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.                               \n",
      "                                                                                     \n",
      "Generation 58 - Current Pareto front scores:\n",
      "                                                                                     \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                     \n",
      "-2\t0.7041618938526156\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.15000000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "                                                                                     \n",
      "Generation 59 - Current Pareto front scores:\n",
      "                                                                                   \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                   \n",
      "-2\t0.7041618938526156\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.15000000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _mate_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                     \n",
      "Generation 60 - Current Pareto front scores:\n",
      "                                                                                   \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                   \n",
      "-2\t0.7041618938526156\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.15000000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                     \n",
      "Generation 61 - Current Pareto front scores:\n",
      "                                                                                   \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                   \n",
      "-2\t0.7041618938526156\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.15000000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                   \n",
      "Generation 62 - Current Pareto front scores:\n",
      "                                                                                   \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                   \n",
      "-2\t0.7041618938526156\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.15000000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                     \n",
      "Generation 63 - Current Pareto front scores:\n",
      "                                                                                   \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                   \n",
      "-2\t0.7041618938526156\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.15000000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                   \n",
      "Generation 64 - Current Pareto front scores:\n",
      "                                                                                   \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                   \n",
      "-2\t0.7041618938526156\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.15000000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by FeatureAgglomeration..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "                                                                                   \n",
      "Generation 65 - Current Pareto front scores:\n",
      "                                                                                   \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                   \n",
      "-2\t0.7041618938526156\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.15000000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                   \n",
      "Generation 66 - Current Pareto front scores:\n",
      "                                                                                   \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                   \n",
      "-2\t0.7041618938526156\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.15000000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 No feature in X meets the variance threshold 0.01000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                   \n",
      "Generation 67 - Current Pareto front scores:\n",
      "                                                                                   \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                   \n",
      "-2\t0.7041618938526156\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.15000000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Skipped pipeline #4796 due to time out. Continuing to the next pipeline.             \n",
      "                                                                                     \n",
      "Generation 68 - Current Pareto front scores:\n",
      "                                                                                   \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                   \n",
      "-2\t0.7041618938526156\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.15000000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                   \n",
      "Generation 69 - Current Pareto front scores:\n",
      "                                                                                   \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                                                   \n",
      "-2\t0.7041618938526156\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.15000000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "                                                                                   \n",
      "Generation 70 - Current Pareto front scores:\n",
      "                                                               \n",
      "-1\t0.6897416316660302\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "                                                               \n",
      "-2\t0.7041618938526156\tExtraTreesClassifier(CombineDFs(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.15000000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=1, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "Imputing missing values in feature set                         \n",
      "Accuracy: 0.7004323656578134\n",
      "Imputing missing values in feature set\n",
      "[[  5   0   2   0   0   0   0   0   2   2   2]\n",
      " [  0  23   0   3   0   0   1   6  20   0  29]\n",
      " [  0   1  32   6   0   0   2   3  14   0  18]\n",
      " [  0   2   0 101   1   1   0   4  26   0  10]\n",
      " [  0   1   1   0   9   0   0   1   5   0   1]\n",
      " [  0   1   0   4   0  24   0   4  10   0   7]\n",
      " [  0   0   0   0   0   0   5   3   7   0   5]\n",
      " [  0   2   1   2   0   2   0  76  31   0  20]\n",
      " [  1   4   2  10   2   1   2  18 415   0  67]\n",
      " [  0   1   0   0   0   0   0   0   3  19   1]\n",
      " [  0   3   2   3   0   3   0  11  88   0 425]]\n",
      "Best Pipeline: Pipeline(steps=[('featureunion',\n",
      "                 FeatureUnion(transformer_list=[('selectfrommodel',\n",
      "                                                 SelectFromModel(estimator=ExtraTreesClassifier(criterion='entropy',\n",
      "                                                                                                max_features=0.15000000000000002),\n",
      "                                                                 threshold=0.1)),\n",
      "                                                ('functiontransformer',\n",
      "                                                 FunctionTransformer(func=<function copy at 0x10497f670>))])),\n",
      "                ('extratreesclassifier',\n",
      "                 ExtraTreesClassifier(max_features=0.6500000000000001))])\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTClassifier(generations=70,#世代数。パイプラインの最適化を実行する試行回数に相当。\n",
    "                      population_size=70,#世代ごとに保持するパイプライン数。\n",
    "                      offspring_size =None,#子孫数。世代ごとに幾つのパイプラインが生成されるかに相当する\n",
    "                      #population_size+generations*offspring_sizeの数だけパイプラインが生成されます。\n",
    "                      \n",
    "                      mutation_rate=0.9,#遺伝的プログラミングアルゴリズムの変異率\n",
    "                      crossover_rate=0.1,#遺伝的プログラミングアルゴリズムのクロスオーバー率\n",
    "                      #mutation_rate+crossover_rateの合計が1になるようにする。\n",
    "                      \n",
    "                      scoring=\"f1\",#作成されたパイプラインの評価指標\n",
    "                      cv = 5,#交差検証を指す。これは個々のモデルの汎用性能を評価するもので今回は5個に分けて学習を行なっていることを指す\n",
    "                      subsample=0.6,#トレーニングデータをどのくらい用いるのか。1.0で全て用いる1.0未満で過学習を抑えることができるかもしれない\n",
    "                      n_jobs=-1,#使用されるCPUの数-1にすると全てのcpuが使用される\n",
    "                      max_time_mins=None,#tpotが最適化を行う最大分数\n",
    "                      max_eval_time_mins=10,#TPOTが単一のパイプラインを評価するために必要な分数。\n",
    "                      random_state=None,#毎回ランダムにするのかどうか\n",
    "                      config_dict=None,#TPOTが最適化プロセスで検索する演算子とパラメーターをカスタマイズ。TPOT light,TPOT MDR,TPOT sparse,Noneの入力が可能\n",
    "                      template=None,#事前定義されたパイプライン構造のテンプレート。\n",
    "                      warm_start=True,\n",
    "                      memory=None,\n",
    "                      use_dask=False,\n",
    "                      periodic_checkpoint_folder=None,\n",
    "                      early_stop=None,\n",
    "                      verbosity=3,\n",
    "                      disable_update_check=False,\n",
    "                      log_file=None,\n",
    "                     )\n",
    "tpot.fit(x_train,y_train)#学習の実行\n",
    "print(\"Accuracy:\",tpot.score(x_test,y_test))\n",
    "print(confusion_matrix(y_test,tpot.predict(x_test)))\n",
    "print(\"Best Pipeline:\",tpot.fitted_pipeline_)#最も評価の高いパイプラインの表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4691be",
   "metadata": {},
   "source": [
    "## 評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "101e1ea7-6307-4355-b13e-6b1524ffb895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#テスト用データの読み込み\n",
    "test = pd.read_csv(test_master_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8c2172e2-5d7f-4a0e-a3b6-1d50169dd771",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "29cf7833-5e67-4ac9-a42b-bbc3bf13a1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4047 entries, 0 to 4046\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   popularity        4047 non-null   int64  \n",
      " 1   duration_ms       4047 non-null   int64  \n",
      " 2   acousticness      4046 non-null   float64\n",
      " 3   positiveness      4033 non-null   float64\n",
      " 4   danceability      4036 non-null   float64\n",
      " 5   loudness          4047 non-null   float64\n",
      " 6   energy            4046 non-null   float64\n",
      " 7   liveness          4041 non-null   float64\n",
      " 8   speechiness       4036 non-null   float64\n",
      " 9   instrumentalness  4045 non-null   float64\n",
      " 10  tempo             4047 non-null   object \n",
      " 11  region            4047 non-null   object \n",
      "dtypes: float64(8), int64(2), object(2)\n",
      "memory usage: 379.5+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "01ef651b-e938-46dd-9c0c-d30b360b21bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4047 entries, 0 to 4046\n",
      "Data columns (total 41 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   popularity        4047 non-null   int64  \n",
      " 1   duration_ms       4047 non-null   int64  \n",
      " 2   acousticness      4046 non-null   float64\n",
      " 3   positiveness      4033 non-null   float64\n",
      " 4   danceability      4036 non-null   float64\n",
      " 5   loudness          4047 non-null   float64\n",
      " 6   energy            4046 non-null   float64\n",
      " 7   liveness          4041 non-null   float64\n",
      " 8   speechiness       4036 non-null   float64\n",
      " 9   instrumentalness  4045 non-null   float64\n",
      " 10  tempo_121-152     4047 non-null   uint8  \n",
      " 11  tempo_153-176     4047 non-null   uint8  \n",
      " 12  tempo_177-192     4047 non-null   uint8  \n",
      " 13  tempo_193-208     4047 non-null   uint8  \n",
      " 14  tempo_209-220     4047 non-null   uint8  \n",
      " 15  tempo_41-50       4047 non-null   uint8  \n",
      " 16  tempo_51-56       4047 non-null   uint8  \n",
      " 17  tempo_57-63       4047 non-null   uint8  \n",
      " 18  tempo_64-76       4047 non-null   uint8  \n",
      " 19  tempo_77-96       4047 non-null   uint8  \n",
      " 20  tempo_97-120      4047 non-null   uint8  \n",
      " 21  region_region_B   4047 non-null   uint8  \n",
      " 22  region_region_C   4047 non-null   uint8  \n",
      " 23  region_region_D   4047 non-null   uint8  \n",
      " 24  region_region_E   4047 non-null   uint8  \n",
      " 25  region_region_F   4047 non-null   uint8  \n",
      " 26  region_region_G   4047 non-null   uint8  \n",
      " 27  region_region_H   4047 non-null   uint8  \n",
      " 28  region_region_I   4047 non-null   uint8  \n",
      " 29  region_region_J   4047 non-null   uint8  \n",
      " 30  region_region_K   4047 non-null   uint8  \n",
      " 31  region_region_L   4047 non-null   uint8  \n",
      " 32  region_region_M   4047 non-null   uint8  \n",
      " 33  region_region_N   4047 non-null   uint8  \n",
      " 34  region_region_O   4047 non-null   uint8  \n",
      " 35  region_region_P   4047 non-null   uint8  \n",
      " 36  region_region_Q   4047 non-null   uint8  \n",
      " 37  region_region_R   4047 non-null   uint8  \n",
      " 38  region_region_S   4047 non-null   uint8  \n",
      " 39  region_region_T   4047 non-null   uint8  \n",
      " 40  region_unknown    4047 non-null   uint8  \n",
      "dtypes: float64(8), int64(2), uint8(31)\n",
      "memory usage: 438.8 KB\n"
     ]
    }
   ],
   "source": [
    "test = pd.get_dummies(test,drop_first=True)\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fb4984cd-c602-42a3-90c3-22cb2f8b932f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing missing values in feature set\n"
     ]
    }
   ],
   "source": [
    "#検証用データの予測\n",
    "pred = tpot.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a8286a8b-c796-4ca0-a43b-996fcad3d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#予測データをデータフレームに格納\n",
    "pred = pd.DataFrame(pred,columns=[\"genre\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4132ec02-45ad-4722-a4ae-368b5b60cdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#idを取得するためにテスト用データを再度読み込み\n",
    "test_id = pd.read_csv(test_master_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a7b0c947-8747-4a2b-bc5c-3acab78343fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#o番目にあるindexを持ってくる\n",
    "test_id  = test_id.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a8feaae1-2226-4837-88cb-3a9bc3ac8d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#先ほどのtest_idとpredを結合する\n",
    "submit = pd.concat([test_id,pred],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "91f00f5b-b502-421f-965b-4d60dfb3ac00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4046</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4047</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4048</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4049</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4050</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4042</th>\n",
       "      <td>8088</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4043</th>\n",
       "      <td>8089</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4044</th>\n",
       "      <td>8090</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4045</th>\n",
       "      <td>8091</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4046</th>\n",
       "      <td>8092</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4047 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  genre\n",
       "0      4046      8\n",
       "1      4047     10\n",
       "2      4048      8\n",
       "3      4049      8\n",
       "4      4050      8\n",
       "...     ...    ...\n",
       "4042   8088      8\n",
       "4043   8089      1\n",
       "4044   8090      8\n",
       "4045   8091      6\n",
       "4046   8092     10\n",
       "\n",
       "[4047 rows x 2 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0b6e36dc-a07f-4ab2-ab23-b60ae34f1f5d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#csvファイルに格納する\n",
    "submit.to_csv(submit_master_path,header=None,index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
